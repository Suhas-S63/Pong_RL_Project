{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reinforcement Learning-Based Pong Game from Atari with Deep Q-Network Agent : A Detailed Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Introduction**\n",
    "\n",
    "This RL project is based on the interative arcade game called Pong created by Allan Alcorn, an engineer at Atari,Inc, an American Video game developer company in 1972. Here, the game is played and learned by an AI agent through Reinforcement Learning techniques. Using the Deep Q-Network(DQN) algorithm, the agent gets trained to control one of the paddles and compete against a human opponent. The agent learns over time progressively, interacting with the environment and updates its policy using the experience it gains from training/playing.\n",
    "\n",
    "Here, a feature is also added for the agent to learn from an human opponent while playing against them in real time. Thus, enhancing the ability of the agent to play through dynamic and continuous learning.\n",
    "\n",
    "\n",
    "### **2. What is Pong game?**\n",
    "Pong is a classic 2D arcade game in which two paddles move vertically on either side of the screen to hit a ball back back and forth continuously. The goal is to score points by sending the ball past the other paddle that is controlled by another opponent.\n",
    "The game mechanics include:\n",
    "- **Ball Movement**: The ball moves with a constant velocity and bounces off the paddles and the top and bottom walls.\n",
    "- **Paddle Movement**: Paddle moves with a constant velocity and vertically to intercept the ball\n",
    "- **Scoring Points**: A point is scored if the any of the players fail to hit the ball and let it pass through the paddle boundaries\n",
    "\n",
    "### **3. Reinforcement Learning (RL) in Pong**\n",
    "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment and receiving rewards or penalties based on the outcomes of these actions. In our Pong game:\n",
    "- the agent's goal is to maximise the cumulative reward by learning how to move the paddle to successfully hit the ball and prevent the opponent from scoring the point.\n",
    "- The only actions here to be performed is the paddle movements i.e up and down\n",
    "- Rewards are given bases Rewards are given based on whether the agent hits the ball (+1), loses a point (-10), or scores (+10).\n",
    "\n",
    "**Reinforcement Learning General Framework**\n",
    "  - **Agent**: An entity that learns and makes decisions based on the observed environment and the rewards it receives.\n",
    "  - **State (S)**: The current state of the environment, which is represented by the ball's position,its velocity, and the paddle position.\n",
    "  - **Action (A)**: The set of actions ( here it is only two actions) available to the agent.\n",
    "  - **Reward (R)**: The feedback recieved from the environment after taking an action from the action pool based on the circumstance/state of the environment. Positive rewards are for success(ex, hitting the ball), negative rewards are for losing a point or letting the ball go.\n",
    "  - **Policy ($\\pi$)**: The policy is a strategy that will be used by the agent to follow a set of actions based on the current state to maximise rewards.\n",
    "  - **Value Function(V)**: The expected cumulative reward that the agent will receive, starting from a given state and following the policy.\n",
    "\n",
    "The Pong game environment for this project is modelled as a Markov Decision Process(MDP) where: \n",
    " - The next state depends on the current state and action taken which is the Markov Property\n",
    " - The agent learns to approximate the **Q-value function** using *Neural Networks*.\n",
    "  \n",
    "### **4. Deep Q-Network (DQN) for Pong Game**\n",
    "This project uses Deep Q-Network(DQN) algorithm to train the AI Agent.\n",
    "\n",
    "#### **4.1. Q-Learning**\n",
    "\n",
    "Q-learning is a form of model-free reinforcement learning where the agent learns a Q-function $ Q(s, a) $, which estimates the value (expected future rewards) of taking action $ a $ in state $ s $. The update rule for Q-learning is:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ r $ is the reward obtained from the environment.\n",
    "- $ \\gamma $ is the discount factor, which determines the importance of future rewards.\n",
    "- $ s' $ is the new state after taking action $ a $.\n",
    "\n",
    "#### **4.2. Deep Q-Network (DQN)**\n",
    "\n",
    "DQN extends Q-learning by using a neural network to approximate the Q-function $ Q(s, a) $. The agent interacts with the environment, collects experiences, and uses these experiences to train a neural network to predict the Q-values.\n",
    "\n",
    "##### **Network Architecture for DQN**\n",
    "The neural network in DQN has the given architecture:\n",
    "- **Input Layer**: The state of the Pong environment (ball position, velocity, paddle positions).\n",
    "- **Hidden Layers**: Several fully connected layers that process the input to extract useful features.\n",
    "- **Output Layer**: Two output values, each representing the Q-value of one possible action (move up or move down).\n",
    "\n",
    "#### **4.3. Experience Replay**\n",
    "\n",
    "One crucial feature of DQN is **experience replay** where instead of updating the Q-values after each action, the agent stores the experiences it gains $ (s, a, r, s') $ in a memory buffer which it periodically samples as a batch of experiences to update the neural network. This helps break the correlation between consecutive experiences and stabilizes training.\n",
    "\n",
    "#### **4.4. Target Network**\n",
    "\n",
    "DQN also uses a **target network** to stabilize the Q-value updates. This target network is a copy of the Q-network, but its weights are updated less frequently which reduces oscillations and divergence during training to maintain a consistent agent and its plays.\n",
    "\n",
    "#### **4.5. Epsilon-Greedy Policy (Exploration vs Explotation condition balance)**\n",
    "\n",
    "To balance exploration and exploitation, the DQN agent follows an **epsilon-greedy policy** where it explores by taking random actions with probability $ \\epsilon $, and exploits its current knowledge (choosing the action with the highest Q-value) with probability $ 1 - \\epsilon $. Over time, $ \\epsilon $ decays to reduce exploration as the agent becomes more confident in its policy.\n",
    "\n",
    "### **5. Tech Stack**\n",
    "\n",
    "- **Turtle Graphics**: Used for rendering the Pong game environment, including paddles, ball movement, and the game screen.\n",
    "- **PyTorch**: To implement the DQN model and for building and training the neural network that approximates the Q-function.\n",
    "- **NumPy**: For numerical operations and handling state representations.\n",
    "- **Collections (Deque)**: For managing experience replay by storing past experiences.\n",
    "\n",
    "### **6. Implementation Procedure**\n",
    "\n",
    "#### **6.1. Game Setup**\n",
    "The game environment is created and setup using Python's Turtle module. The screen is initialized, and the paddles and ball are rendered. The game mechanics are coded, including ball bouncing, scoring, and paddle control.\n",
    "\n",
    "#### **6.2. Agent Training**\n",
    "- The DQN agent interacts with the Pong environment, where the state is represented by a vector of six values (ball position, ball velocity, left paddle position, right paddle position).\n",
    "- The agent receives rewards based on performance of its actions and the impacts it has on the environment (e.g., hitting the ball or scoring points).\n",
    "- Over the iteration of episodes, the agent's Q-network is updated using the Q-learning update rule, where the expected future reward is approximated through a neural network.\n",
    "\n",
    "#### **6.3. Playing Against the Agent**\n",
    "After training the agent, the model is saved and can be loaded for gameplay. The human controls the right paddle using keyboard inputs, while the trained agent controls the left paddle. The agent continues to learn during the game using experience replay, improving its performance as it plays against the human."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Primary Equations Used**\n",
    "\n",
    "#### **Q-Learning Update Rule**:\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ Q(s, a) $: Current Q-value for state $ s $ and action $ a $.\n",
    "- $ r $: Immediate reward from the environment.\n",
    "- $ \\gamma $: Discount factor, controlling the weight of future rewards.\n",
    "- $ \\alpha $: Learning rate.\n",
    "\n",
    "#### **DQN Loss Function**:\n",
    "The goal is to minimize the difference between the predicted Q-values (from the neural network) and the target Q-values (which represent the \"true\" value the agent should have learned for a particular state-action pair). This difference is captured using the Mean Squared Error (MSE).\n",
    "The loss function used to update the neural network is the mean squared error (MSE) between the predicted Q-values and the target Q-values:\n",
    "\n",
    "$$\n",
    "L = \\mathbb{E} \\left[ \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "- **$ L $**: This is the loss, specifically the mean squared error between the predicted Q-values and the target Q-values.\n",
    "\n",
    "- **$ \\mathbb{E} $**: Represents the expected value, or the average over a batch of experiences sampled from the replay memory.\n",
    "\n",
    "- **$ r $**: The **immediate reward** received after taking action $ a $ in state $ s $. This is the short-term feedback the agent gets from the environment after its action.\n",
    "\n",
    "- **$ \\gamma $**: The **discount factor**, which determines the importance of future rewards. A value between 0 and 1, where:\n",
    "  - $ \\gamma = 0 $ means the agent only cares about immediate rewards.\n",
    "  - $ \\gamma $ close to 1 means the agent values future rewards more, encouraging it to plan ahead.\n",
    "\n",
    "- **$ \\max_{a'} Q(s', a') $**: The maximum predicted Q-value for the **next state** $ s' $ across all possible next actions $ a' $. This represents the agent's estimate of the best future reward it can obtain from the next state.\n",
    "\n",
    "- **$ Q(s, a) $**: The **predicted Q-value** for taking action $ a $ in the current state $ s $.\n",
    "\n",
    "- **$ r + \\gamma \\max_{a'} Q(s', a') $**: This term represents the **target Q-value**. It's the immediate reward $ r $ plus the discounted maximum future reward $ \\max_{a'} Q(s', a') $. This is the \"true\" Q-value that the neural network is trying to approximate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Challenges Encountered and Steps taken**\n",
    "\n",
    "1. **Exploration-Exploitation Tradeoff**: A balance between exploring new actions and exploiting known strategies had to be achieved by using an epsilon-greedy policy.\n",
    "2. **Training Stability**: Using experience replay and a target network was necessary to stabilize the learning process.\n",
    "3. **Human-AI Interaction**: Had to ensure a  smooth gameplay between the human and AI agent required balancing the game's speed, AI responsiveness, and the agent's continuous learning during gameplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import turtle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pong Environment ---\n",
    "\n",
    "class PongEnv:\n",
    "    def __init__(self):\n",
    "        # Initialize the screen\n",
    "        self.screen = turtle.Screen()\n",
    "        self.screen.title(\"Pong RL\")\n",
    "        self.screen.bgcolor(\"white\")\n",
    "        self.screen.setup(width=1000, height=600)\n",
    "\n",
    "        # Left paddle (controlled by the DQN agent)\n",
    "        self.left_paddle = turtle.Turtle()\n",
    "        self.left_paddle.speed(0)\n",
    "        self.left_paddle.shape(\"square\")\n",
    "        self.left_paddle.color(\"black\")\n",
    "        self.left_paddle.shapesize(stretch_wid=6, stretch_len=2)\n",
    "        self.left_paddle.penup()\n",
    "        self.left_paddle.goto(-400, 0)\n",
    "\n",
    "        # Right paddle (controlled by simple AI)\n",
    "        self.right_paddle = turtle.Turtle()\n",
    "        self.right_paddle.speed(0)\n",
    "        self.right_paddle.shape(\"square\")\n",
    "        self.right_paddle.color(\"black\")\n",
    "        self.right_paddle.shapesize(stretch_wid=6, stretch_len=2)\n",
    "        self.right_paddle.penup()\n",
    "        self.right_paddle.goto(400, 0)\n",
    "\n",
    "        # Ball\n",
    "        self.ball = turtle.Turtle()\n",
    "        self.ball.speed(40)\n",
    "        self.ball.shape(\"circle\")\n",
    "        self.ball.color(\"blue\")\n",
    "        self.ball.penup()\n",
    "        self.ball.goto(0, 0)\n",
    "        self.ball.dx = 5  # Horizontal velocity\n",
    "        self.ball.dy = -5  # Vertical velocity\n",
    "\n",
    "        # Score\n",
    "        self.left_score = 0\n",
    "        self.right_score = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset the ball position and randomly assign a new direction \"\"\"\n",
    "        self.ball.goto(0, 0)\n",
    "        self.ball.dx *= random.choice([-1, 1])\n",
    "        self.ball.dy *= random.choice([-1, 1])\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\" Get the current state: [ball_x, ball_y, ball_dx, ball_dy, left_paddle_y, right_paddle_y] \"\"\"\n",
    "        return np.array([self.ball.xcor(), self.ball.ycor(),\n",
    "                         self.ball.dx, self.ball.dy,\n",
    "                         self.left_paddle.ycor(), self.right_paddle.ycor()])\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" \n",
    "        Perform the selected action and move the paddles and ball accordingly.\n",
    "        Return the new state, reward, and whether the episode has ended (done).\n",
    "        \"\"\"\n",
    "        # Move left paddle based on the agent's action (0 = move up, 1 = move down)\n",
    "        if action == 0 and self.left_paddle.ycor() < 250:\n",
    "            self.left_paddle.sety(self.left_paddle.ycor() + 20)\n",
    "        elif action == 1 and self.left_paddle.ycor() > -240:\n",
    "            self.left_paddle.sety(self.left_paddle.ycor() - 20)\n",
    "\n",
    "        # Simple AI for the right paddle (follows the ball)\n",
    "        if self.right_paddle.ycor() < self.ball.ycor() and self.right_paddle.ycor() < 250:\n",
    "            self.right_paddle.sety(self.right_paddle.ycor() + 20)\n",
    "        elif self.right_paddle.ycor() > self.ball.ycor() and self.right_paddle.ycor() > -240:\n",
    "            self.right_paddle.sety(self.right_paddle.ycor() - 20)\n",
    "\n",
    "        # Move the ball\n",
    "        self.ball.setx(self.ball.xcor() + self.ball.dx)\n",
    "        self.ball.sety(self.ball.ycor() + self.ball.dy)\n",
    "\n",
    "        # Ball collision with top and bottom walls\n",
    "        if self.ball.ycor() > 290:\n",
    "            self.ball.sety(290)\n",
    "            self.ball.dy *= -1\n",
    "\n",
    "        if self.ball.ycor() < -290:\n",
    "            self.ball.sety(-290)\n",
    "            self.ball.dy *= -1\n",
    "\n",
    "        # Ball collision with paddles\n",
    "        if (self.ball.xcor() > 360 and self.ball.xcor() < 370) and \\\n",
    "        (self.ball.ycor() < self.right_paddle.ycor() + 50 and self.ball.ycor() > self.right_paddle.ycor() - 50):\n",
    "            self.ball.setx(360)\n",
    "            self.ball.dx *= -1  # Reverse direction upon hitting the right paddle\n",
    "\n",
    "        if (self.ball.xcor() < -360 and self.ball.xcor() > -370) and \\\n",
    "        (self.ball.ycor() < self.left_paddle.ycor() + 50 and self.ball.ycor() > self.left_paddle.ycor() - 50):\n",
    "            self.ball.setx(-360)\n",
    "            self.ball.dx *= -1  # Reverse direction upon hitting the left paddle\n",
    "\n",
    "        # Reward for paddle positioning (intermediate reward)\n",
    "        paddle_position_reward = 1 - abs(self.left_paddle.ycor() - self.ball.ycor()) / 300  # Reward based on distance between paddle and ball\n",
    "\n",
    "        # Check for scoring\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Left paddle scores\n",
    "        if self.ball.xcor() > 500:\n",
    "            self.left_score += 1\n",
    "            reward = 10  # Positive reward for scoring\n",
    "            done = True  # End of episode (for training)\n",
    "\n",
    "        # Right paddle scores\n",
    "        if self.ball.xcor() < -500:\n",
    "            self.right_score += 1\n",
    "            reward = -10  # Penalty for opponent scoring\n",
    "            done = True  # End of episode (for training)\n",
    "\n",
    "        # Combine rewards (shaping reward + intermediate reward)\n",
    "        reward += paddle_position_reward  # Add positioning reward to the total reward\n",
    "\n",
    "        # Get the new state after the step\n",
    "        state = self.get_state()\n",
    "        return state, reward, done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DQN Agent ---\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        # Define the neural network layers\n",
    "        self.fc1 = nn.Linear(state_dim, 128)  # Input layer (state_dim -> 128 units)\n",
    "        self.fc2 = nn.Linear(128, 128)        # Hidden layer (128 -> 128 units)\n",
    "        self.fc3 = nn.Linear(128, action_dim) # Output layer (128 -> action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the network\n",
    "        x = F.relu(self.fc1(x))  # ReLU activation\n",
    "        x = F.relu(self.fc2(x))  # ReLU activation\n",
    "        return self.fc3(x)       # Output action values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept of Prioritized Experience Replay (PER)\n",
    "\n",
    "Prioritized Experience Replay (PER) is an improvement to the standard experience replay mechanism used in DQN which aims to improve learning rate of the agent efficiently. \n",
    "In the standard experience replay system, the agent stores the experiences (state, action, reward, next state, done) into a buffer and samples random mini-batches from this buffer to train the neural network. But, this random sampling treats all experiences equally and its not optimal for our case.\n",
    "Priority-based sampling mechanism  increases the learning efficiency by considering only the most \"important\" experiences, which is where the agent has learnt the most. This is improved using the **Temporal Difference (TD) error**, which measures the difference between the predicted Q-value and the target Q-value. Experiences with a higher TD-error indicates that the agent's prediction is high and it has to learn from experiences more to reduce this error. \n",
    "\n",
    "#### Key Concepts:\n",
    "1. **Temporal Difference (TD) Error**:\n",
    "   - TD error is the difference between the Q-value predicted by the Q-network and the target Q-value. It reflects how much the agent's prediction deviates from the actual outcome.\n",
    "   - TD error = |Q(s, a) - (r + γ * max Q(s', a'))|\n",
    "   - Larger TD errors imply that the agent made a larger prediction error, signaling that these experiences are more valuable for learning.\n",
    "\n",
    "2. **Prioritized Sampling**:\n",
    "   - Instead of sampling experiences randomly, PER assigns a priority to each experience based on its TD error. Experiences with larger TD errors are more likely to be sampled for training because they offer a higher potential for reducing the agent's prediction error.\n",
    "   - The probability of sampling an experience $i$ is proportional to its priority $p_i$:\n",
    "     $$\n",
    "     P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}\n",
    "     $$\n",
    "     \n",
    "     where $\\alpha$ controls the degree of prioritization (when $ \\alpha $ = 0, prioritization follows uniform random sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.q_network = DQN(state_dim, action_dim)  # Primary Q-network\n",
    "        self.target_network = DQN(state_dim, action_dim)  # Target Q-network\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)  # Optimizer for training\n",
    "\n",
    "        self.replay_buffer = deque(maxlen=10000)  # Experience replay buffer\n",
    "        self.priorities = deque(maxlen=10000)     # Store TD-errors as priorities for prioritized replay\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99  # Discount factor for future rewards\n",
    "        self.epsilon = 1.0  # Exploration rate (initially explore)\n",
    "        self.epsilon_decay = 0.995  # Decay rate for epsilon\n",
    "        self.epsilon_min = 0.01  # Minimum value for epsilon\n",
    "        self.update_target_steps = 100  # How often to update the target network\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\" Select an action using epsilon-greedy exploration \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, 1)  # Explore (random action)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)  # Convert state to tensor\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state)  # Get action values from the Q-network\n",
    "            return q_values.argmax().item()  # Exploit (select best action)\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\" Sample a batch from replay buffer based on priorities and update the Q-network \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return  # Not enough samples to update\n",
    "\n",
    "        # Normalize priorities to create a probability distribution\n",
    "        priorities = np.array(self.priorities)\n",
    "        probabilities = priorities / priorities.sum()\n",
    "\n",
    "        # Sample experiences based on the calculated probabilities\n",
    "        indices = np.random.choice(len(self.replay_buffer), self.batch_size, p=probabilities)\n",
    "        batch = [self.replay_buffer[i] for i in indices]\n",
    "        \n",
    "        # Convert batch into tensors\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Calculate Q-values and targets\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_network(next_states).max(1)[0]\n",
    "        targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # Calculate loss and update the Q-network\n",
    "        loss = F.mse_loss(q_values, targets.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update the target network periodically\n",
    "        self.steps_done += 1\n",
    "        if self.steps_done % self.update_target_steps == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def add_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Add experience to the replay buffer with its TD-error (priority) \"\"\"\n",
    "        # Calculate the TD-error (temporal difference)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_value = self.q_network(state_tensor)[0, action].item()\n",
    "            next_q_value = self.target_network(next_state_tensor).max(1)[0].item()\n",
    "        \n",
    "        target = reward + (1 - done) * self.gamma * next_q_value\n",
    "        td_error = abs(target - q_value)\n",
    "        \n",
    "        # Append experience and its TD-error to the buffers\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(td_error + 0.01)  # Small value added to avoid zero probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training ---\n",
    "\n",
    "def train_dqn_agent():\n",
    "    # initiating environment and agent\n",
    "    env = PongEnv()\n",
    "    state_dim = 6  # State: [ball_x, ball_y, ball_dx, ball_dy, left_paddle_y, right_paddle_y]\n",
    "    action_dim = 2  # Actions: 0 = move up, 1 = move down\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "    num_episodes = 1000\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # agent selects an action\n",
    "            action = agent.select_action(state)\n",
    "\n",
    "            # environment responds to the action (state is changed)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # gained experience added to replay buffer and agent is updated\n",
    "            agent.add_experience(state, action, reward, next_state, done)\n",
    "            agent.update()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        # Decay epsilon (exploration rate) after each episode to decrease the exploration as its policy is refined\n",
    "        agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suhas\\AppData\\Local\\Temp\\ipykernel_10652\\428573313.py:42: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states = torch.FloatTensor(states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 34.76666666666666\n",
      "Episode 2: Total Reward = -1.0333333333333332\n",
      "Episode 3: Total Reward = 90.53333333333333\n",
      "Episode 4: Total Reward = 103.59999999999997\n",
      "Episode 5: Total Reward = 154.93333333333325\n",
      "Episode 6: Total Reward = 100.03333333333335\n",
      "Episode 7: Total Reward = 78.73333333333335\n",
      "Episode 8: Total Reward = -5.233333333333333\n",
      "Episode 9: Total Reward = 41.16666666666666\n",
      "Episode 10: Total Reward = 75.66666666666667\n",
      "Episode 11: Total Reward = 71.50000000000004\n",
      "Episode 12: Total Reward = 63.09999999999999\n",
      "Episode 13: Total Reward = -40.233333333333334\n",
      "Episode 14: Total Reward = 119.56666666666672\n",
      "Episode 15: Total Reward = 159.9999999999999\n",
      "Episode 16: Total Reward = 41.1\n",
      "Episode 17: Total Reward = -22.63333333333334\n",
      "Episode 18: Total Reward = -8.03333333333333\n",
      "Episode 19: Total Reward = 14.1\n",
      "Episode 20: Total Reward = 80.86666666666662\n",
      "Episode 21: Total Reward = 60.66666666666673\n",
      "Episode 22: Total Reward = 158.46666666666658\n",
      "Episode 23: Total Reward = 117.63333333333338\n",
      "Episode 24: Total Reward = -35.16666666666666\n",
      "Episode 25: Total Reward = 41.66666666666662\n",
      "Episode 26: Total Reward = -9.033333333333339\n",
      "Episode 27: Total Reward = 54.33333333333334\n",
      "Episode 28: Total Reward = 137.2\n",
      "Episode 29: Total Reward = 131.9000000000001\n",
      "Episode 30: Total Reward = -27.23333333333333\n",
      "Episode 31: Total Reward = -21.96666666666667\n",
      "Episode 32: Total Reward = 123.50000000000013\n",
      "Episode 33: Total Reward = 74.53333333333336\n",
      "Episode 34: Total Reward = 111.96666666666664\n",
      "Episode 35: Total Reward = 110.93333333333344\n",
      "Episode 36: Total Reward = 127.30000000000001\n",
      "Episode 37: Total Reward = -33.833333333333336\n",
      "Episode 38: Total Reward = 77.36666666666666\n",
      "Episode 39: Total Reward = -25.833333333333336\n",
      "Episode 40: Total Reward = 137.23333333333335\n",
      "Episode 41: Total Reward = 61.433333333333316\n",
      "Episode 42: Total Reward = 64.7333333333333\n",
      "Episode 43: Total Reward = -28.033333333333335\n",
      "Episode 44: Total Reward = 56.43333333333335\n",
      "Episode 45: Total Reward = -29.03333333333333\n",
      "Episode 46: Total Reward = 86.16666666666674\n",
      "Episode 47: Total Reward = 47.09999999999995\n",
      "Episode 48: Total Reward = 125.0\n",
      "Episode 49: Total Reward = 66.69999999999997\n",
      "Episode 50: Total Reward = 22.366666666666678\n",
      "Episode 51: Total Reward = -11.766666666666667\n",
      "Episode 52: Total Reward = 173.7333333333333\n",
      "Episode 53: Total Reward = 119.53333333333333\n",
      "Episode 54: Total Reward = 6.1\n",
      "Episode 55: Total Reward = 163.36666666666665\n",
      "Episode 56: Total Reward = 68.90000000000006\n",
      "Episode 57: Total Reward = 56.83333333333333\n",
      "Episode 58: Total Reward = 59.16666666666667\n",
      "Episode 59: Total Reward = 47.86666666666666\n",
      "Episode 60: Total Reward = 266.7999999999997\n",
      "Episode 61: Total Reward = -8.366666666666667\n",
      "Episode 62: Total Reward = 49.099999999999994\n",
      "Episode 63: Total Reward = -37.76666666666666\n",
      "Episode 64: Total Reward = 51.53333333333332\n",
      "Episode 65: Total Reward = 131.60000000000002\n",
      "Episode 66: Total Reward = 132.20000000000007\n",
      "Episode 67: Total Reward = 56.0333333333333\n",
      "Episode 68: Total Reward = -34.833333333333336\n",
      "Episode 69: Total Reward = 160.4999999999999\n",
      "Episode 70: Total Reward = 59.53333333333336\n",
      "Episode 71: Total Reward = -28.433333333333334\n",
      "Episode 72: Total Reward = 72.40000000000002\n",
      "Episode 73: Total Reward = 49.19999999999995\n",
      "Episode 74: Total Reward = 49.43333333333334\n",
      "Episode 75: Total Reward = 61.96666666666667\n",
      "Episode 76: Total Reward = -32.03333333333333\n",
      "Episode 77: Total Reward = 78.23333333333332\n",
      "Episode 78: Total Reward = -38.96666666666667\n",
      "Episode 79: Total Reward = -40.7\n",
      "Episode 80: Total Reward = 57.00000000000003\n",
      "Episode 81: Total Reward = 139.0333333333333\n",
      "Episode 82: Total Reward = 132.73333333333343\n",
      "Episode 83: Total Reward = 66.06666666666676\n",
      "Episode 84: Total Reward = 44.06666666666666\n",
      "Episode 85: Total Reward = 47.3\n",
      "Episode 86: Total Reward = 61.76666666666665\n",
      "Episode 87: Total Reward = -41.96666666666667\n",
      "Episode 88: Total Reward = 44.66666666666667\n",
      "Episode 89: Total Reward = 44.400000000000006\n",
      "Episode 90: Total Reward = 70.43333333333335\n",
      "Episode 91: Total Reward = 1.8333333333333321\n",
      "Episode 92: Total Reward = 44.36666666666667\n",
      "Episode 93: Total Reward = 4.966666666666667\n",
      "Episode 94: Total Reward = 130.70000000000002\n",
      "Episode 95: Total Reward = 21.799999999999986\n",
      "Episode 96: Total Reward = 112.5\n",
      "Episode 97: Total Reward = -42.233333333333334\n",
      "Episode 98: Total Reward = 37.03333333333324\n",
      "Episode 99: Total Reward = 150.00000000000009\n",
      "Episode 100: Total Reward = 129.63333333333333\n",
      "Episode 101: Total Reward = -43.10000000000001\n",
      "Episode 102: Total Reward = 66.8\n",
      "Episode 103: Total Reward = 182.1666666666667\n",
      "Episode 104: Total Reward = 159.59999999999988\n",
      "Episode 105: Total Reward = 188.5666666666667\n",
      "Episode 106: Total Reward = -23.96666666666667\n",
      "Episode 107: Total Reward = 47.10000000000001\n",
      "Episode 108: Total Reward = 77.46666666666665\n",
      "Episode 109: Total Reward = 170.99999999999983\n",
      "Episode 110: Total Reward = 168.53333333333336\n",
      "Episode 111: Total Reward = 17.63333333333334\n",
      "Episode 112: Total Reward = 166.00000000000006\n",
      "Episode 113: Total Reward = 45.266666666666644\n",
      "Episode 114: Total Reward = 155.63333333333347\n",
      "Episode 115: Total Reward = -44.43333333333333\n",
      "Episode 116: Total Reward = 148.8333333333334\n",
      "Episode 117: Total Reward = 116.46666666666663\n",
      "Episode 118: Total Reward = 42.566666666666656\n",
      "Episode 119: Total Reward = -41.766666666666666\n",
      "Episode 120: Total Reward = -40.96666666666667\n",
      "Episode 121: Total Reward = 42.533333333333346\n",
      "Episode 122: Total Reward = -41.166666666666664\n",
      "Episode 123: Total Reward = 40.93333333333328\n",
      "Episode 124: Total Reward = -40.166666666666664\n",
      "Episode 125: Total Reward = 55.36666666666668\n",
      "Episode 126: Total Reward = 111.66666666666666\n",
      "Episode 127: Total Reward = 59.900000000000006\n",
      "Episode 128: Total Reward = 72.20000000000002\n",
      "Episode 129: Total Reward = -39.3\n",
      "Episode 130: Total Reward = 116.60000000000002\n",
      "Episode 131: Total Reward = 118.66666666666671\n",
      "Episode 132: Total Reward = 111.63333333333328\n",
      "Episode 133: Total Reward = 38.63333333333334\n",
      "Episode 134: Total Reward = 41.73333333333334\n",
      "Episode 135: Total Reward = 43.7333333333334\n",
      "Episode 136: Total Reward = 40.099999999999994\n",
      "Episode 137: Total Reward = 40.96666666666667\n",
      "Episode 138: Total Reward = 40.399999999999984\n",
      "Episode 139: Total Reward = 42.69999999999998\n",
      "Episode 140: Total Reward = 41.900000000000006\n",
      "Episode 141: Total Reward = 116.79999999999991\n",
      "Episode 142: Total Reward = -41.03333333333333\n",
      "Episode 143: Total Reward = 39.03333333333333\n",
      "Episode 144: Total Reward = 117.80000000000001\n",
      "Episode 145: Total Reward = 31.299999999999983\n",
      "Episode 146: Total Reward = 35.66666666666666\n",
      "Episode 147: Total Reward = 28.06666666666665\n",
      "Episode 148: Total Reward = 32.3\n",
      "Episode 149: Total Reward = 42.43333333333335\n",
      "Episode 150: Total Reward = 23.26666666666666\n",
      "Episode 151: Total Reward = 121.66666666666676\n",
      "Episode 152: Total Reward = 45.699999999999946\n",
      "Episode 153: Total Reward = 44.63333333333342\n",
      "Episode 154: Total Reward = -42.83333333333334\n",
      "Episode 155: Total Reward = -40.16666666666667\n",
      "Episode 156: Total Reward = 26.866666666666667\n",
      "Episode 157: Total Reward = -41.89999999999999\n",
      "Episode 158: Total Reward = -41.099999999999994\n",
      "Episode 159: Total Reward = 118.83333333333329\n",
      "Episode 160: Total Reward = 48.96666666666662\n",
      "Episode 161: Total Reward = 49.66666666666668\n",
      "Episode 162: Total Reward = 105.86666666666682\n",
      "Episode 163: Total Reward = 33.3333333333333\n",
      "Episode 164: Total Reward = 76.09999999999995\n",
      "Episode 165: Total Reward = 86.00000000000003\n",
      "Episode 166: Total Reward = 34.46666666666667\n",
      "Episode 167: Total Reward = 56.89999999999997\n",
      "Episode 168: Total Reward = 86.69999999999996\n",
      "Episode 169: Total Reward = 50.99999999999997\n",
      "Episode 170: Total Reward = -14.433333333333337\n",
      "Episode 171: Total Reward = 13.533333333333337\n",
      "Episode 172: Total Reward = 1.0\n",
      "Episode 173: Total Reward = 8.099999999999994\n",
      "Episode 174: Total Reward = 11.966666666666665\n",
      "Episode 175: Total Reward = 15.099999999999998\n",
      "Episode 176: Total Reward = 1.266666666666664\n",
      "Episode 177: Total Reward = 48.4\n",
      "Episode 178: Total Reward = 103.26666666666671\n",
      "Episode 179: Total Reward = 117.69999999999992\n",
      "Episode 180: Total Reward = 114.23333333333333\n",
      "Episode 181: Total Reward = -2.666666666666665\n",
      "Episode 182: Total Reward = 91.93333333333337\n",
      "Episode 183: Total Reward = 105.86666666666666\n",
      "Episode 184: Total Reward = -5.366666666666667\n",
      "Episode 185: Total Reward = 114.3666666666667\n",
      "Episode 186: Total Reward = 200.30000000000004\n",
      "Episode 187: Total Reward = 187.46666666666658\n",
      "Episode 188: Total Reward = 59.566666666666656\n",
      "Episode 189: Total Reward = 294.9999999999998\n",
      "Episode 190: Total Reward = 304.8333333333331\n",
      "Episode 191: Total Reward = 308.00000000000006\n",
      "Episode 192: Total Reward = 40.233333333333334\n",
      "Episode 193: Total Reward = 127.36666666666672\n",
      "Episode 194: Total Reward = 197.33333333333326\n",
      "Episode 195: Total Reward = 39.099999999999994\n",
      "Episode 196: Total Reward = 31.699999999999996\n",
      "Episode 197: Total Reward = 192.73333333333338\n",
      "Episode 198: Total Reward = 160.30000000000007\n",
      "Episode 199: Total Reward = 97.70000000000003\n",
      "Episode 200: Total Reward = 100.93333333333337\n",
      "Episode 201: Total Reward = 25.59999999999998\n",
      "Episode 202: Total Reward = 43.5\n",
      "Episode 203: Total Reward = 772.8499999999988\n",
      "Episode 204: Total Reward = 140.13333333333335\n",
      "Episode 205: Total Reward = 208.96666666666664\n",
      "Episode 206: Total Reward = 197.13333333333327\n",
      "Episode 207: Total Reward = 171.46666666666664\n",
      "Episode 208: Total Reward = 216.2333333333334\n",
      "Episode 209: Total Reward = 560.7833333333335\n",
      "Episode 210: Total Reward = 232.39999999999992\n",
      "Episode 211: Total Reward = -36.56666666666667\n",
      "Episode 212: Total Reward = 172.83333333333337\n",
      "Episode 213: Total Reward = 108.50000000000006\n",
      "Episode 214: Total Reward = 14.8\n",
      "Episode 215: Total Reward = 148.06666666666666\n",
      "Episode 216: Total Reward = 182.3999999999999\n",
      "Episode 217: Total Reward = 130.06666666666683\n",
      "Episode 218: Total Reward = 205.09999999999994\n",
      "Episode 219: Total Reward = 165.00000000000009\n",
      "Episode 220: Total Reward = 113.6333333333334\n",
      "Episode 221: Total Reward = 202.03333333333336\n",
      "Episode 222: Total Reward = 232.99999999999986\n",
      "Episode 223: Total Reward = 154.0333333333333\n",
      "Episode 224: Total Reward = 18.53333333333334\n",
      "Episode 225: Total Reward = 213.13333333333327\n",
      "Episode 226: Total Reward = 135.86666666666662\n",
      "Episode 227: Total Reward = 268.33333333333314\n",
      "Episode 228: Total Reward = 221.10000000000002\n",
      "Episode 229: Total Reward = 183.73333333333323\n",
      "Episode 230: Total Reward = 120.90000000000008\n",
      "Episode 231: Total Reward = -18.833333333333332\n",
      "Episode 232: Total Reward = 204.89999999999984\n",
      "Episode 233: Total Reward = 301.5999999999998\n",
      "Episode 234: Total Reward = 253.3999999999998\n",
      "Episode 235: Total Reward = 257.2666666666666\n",
      "Episode 236: Total Reward = 141.3999999999999\n",
      "Episode 237: Total Reward = 204.89999999999986\n",
      "Episode 238: Total Reward = 259.2\n",
      "Episode 239: Total Reward = 166.59999999999997\n",
      "Episode 240: Total Reward = 229.06666666666666\n",
      "Episode 241: Total Reward = 170.86666666666636\n",
      "Episode 242: Total Reward = 288.1666666666663\n",
      "Episode 243: Total Reward = 174.13333333333327\n",
      "Episode 244: Total Reward = 343.39999999999986\n",
      "Episode 245: Total Reward = 414.5666666666663\n",
      "Episode 246: Total Reward = 132.09999999999994\n",
      "Episode 247: Total Reward = 671.8499999999999\n",
      "Episode 248: Total Reward = 335.09999999999974\n",
      "Episode 249: Total Reward = 300.29999999999967\n",
      "Episode 250: Total Reward = 282.2333333333331\n",
      "Episode 251: Total Reward = 276.0999999999998\n",
      "Episode 252: Total Reward = 243.49999999999991\n",
      "Episode 253: Total Reward = 260.4666666666666\n",
      "Episode 254: Total Reward = 252.8\n",
      "Episode 255: Total Reward = 272.13333333333304\n",
      "Episode 256: Total Reward = 239.2666666666666\n",
      "Episode 257: Total Reward = 211.83333333333323\n",
      "Episode 258: Total Reward = 201.99999999999991\n",
      "Episode 259: Total Reward = 488.16666666666634\n",
      "Episode 260: Total Reward = 198.36666666666653\n",
      "Episode 261: Total Reward = 13.266666666666675\n",
      "Episode 262: Total Reward = 79.7\n",
      "Episode 263: Total Reward = 577.6499999999994\n",
      "Episode 264: Total Reward = 134.00000000000009\n",
      "Episode 265: Total Reward = 242.03333333333327\n",
      "Episode 266: Total Reward = 301.3999999999999\n",
      "Episode 267: Total Reward = 184.1333333333333\n",
      "Episode 268: Total Reward = -13.399999999999999\n",
      "Episode 269: Total Reward = 209.03333333333327\n",
      "Episode 270: Total Reward = 143.16666666666666\n",
      "Episode 271: Total Reward = 47.73333333333333\n",
      "Episode 272: Total Reward = 608.4833333333331\n",
      "Episode 273: Total Reward = 713.2166666666662\n",
      "Episode 274: Total Reward = 262.39999999999986\n",
      "Episode 275: Total Reward = 35.66666666666666\n",
      "Episode 276: Total Reward = 319.06666666666644\n",
      "Episode 277: Total Reward = 580.1833333333335\n",
      "Episode 278: Total Reward = 635.8166666666665\n",
      "Episode 279: Total Reward = 617.5500000000003\n",
      "Episode 280: Total Reward = 49.73333333333335\n",
      "Episode 281: Total Reward = 1017.7166666666665\n",
      "Episode 282: Total Reward = 1282.1166666666695\n",
      "Episode 283: Total Reward = 915.8833333333329\n",
      "Episode 284: Total Reward = 218.8333333333332\n",
      "Episode 285: Total Reward = 9.233333333333313\n",
      "Episode 286: Total Reward = 459.2000000000001\n",
      "Episode 287: Total Reward = 73.29999999999995\n",
      "Episode 288: Total Reward = 150.8\n",
      "Episode 289: Total Reward = 694.2166666666672\n",
      "Episode 290: Total Reward = 225.73333333333323\n",
      "Episode 291: Total Reward = 416.9666666666666\n",
      "Episode 292: Total Reward = 755.2833333333332\n",
      "Episode 293: Total Reward = 119.36666666666666\n",
      "Episode 294: Total Reward = 892.3500000000006\n",
      "Episode 295: Total Reward = 339.93333333333317\n",
      "Episode 296: Total Reward = 48.56666666666666\n",
      "Episode 297: Total Reward = 390.8\n",
      "Episode 298: Total Reward = 52.06666666666666\n",
      "Episode 299: Total Reward = 37.63333333333332\n",
      "Episode 300: Total Reward = 73.76666666666662\n",
      "Episode 301: Total Reward = 62.800000000000004\n",
      "Episode 302: Total Reward = 86.93333333333332\n",
      "Episode 303: Total Reward = 124.23333333333325\n",
      "Episode 304: Total Reward = 254.26666666666668\n",
      "Episode 305: Total Reward = 788.8166666666666\n",
      "Episode 306: Total Reward = 515.333333333333\n",
      "Episode 307: Total Reward = 609.2833333333331\n",
      "Episode 308: Total Reward = 383.133333333333\n",
      "Episode 309: Total Reward = 59.93333333333332\n",
      "Episode 310: Total Reward = 16.000000000000004\n",
      "Episode 311: Total Reward = 273.1833333333332\n",
      "Episode 312: Total Reward = 125.33333333333327\n",
      "Episode 313: Total Reward = 565.6166666666661\n",
      "Episode 314: Total Reward = -3.1666666666666723\n",
      "Episode 315: Total Reward = 189.73333333333323\n",
      "Episode 316: Total Reward = 42.63333333333334\n",
      "Episode 317: Total Reward = 543.9499999999998\n",
      "Episode 318: Total Reward = 31.000000000000007\n",
      "Episode 319: Total Reward = 392.59999999999985\n",
      "Episode 320: Total Reward = 174.2666666666668\n",
      "Episode 321: Total Reward = 16.666666666666664\n",
      "Episode 322: Total Reward = 157.56666666666672\n",
      "Episode 323: Total Reward = 58.16666666666668\n",
      "Episode 324: Total Reward = 87.76666666666668\n",
      "Episode 325: Total Reward = 204.50000000000003\n",
      "Episode 326: Total Reward = 528.4833333333332\n",
      "Episode 327: Total Reward = 117.26666666666674\n",
      "Episode 328: Total Reward = 50.3\n",
      "Episode 329: Total Reward = 26.299999999999994\n",
      "Episode 330: Total Reward = 56.70000000000001\n",
      "Episode 331: Total Reward = 61.93333333333332\n",
      "Episode 332: Total Reward = 390.65000000000003\n",
      "Episode 333: Total Reward = 250.2166666666663\n",
      "Episode 334: Total Reward = 47.96666666666666\n",
      "Episode 335: Total Reward = 57.76666666666667\n",
      "Episode 336: Total Reward = 13.599999999999994\n",
      "Episode 337: Total Reward = 107.53333333333333\n",
      "Episode 338: Total Reward = 115.6333333333333\n",
      "Episode 339: Total Reward = 106.10000000000002\n",
      "Episode 340: Total Reward = 49.33333333333331\n",
      "Episode 341: Total Reward = 12.133333333333336\n",
      "Episode 342: Total Reward = -20.9\n",
      "Episode 343: Total Reward = 65.13333333333334\n",
      "Episode 344: Total Reward = 139.53333333333333\n",
      "Episode 345: Total Reward = 45.233333333333306\n",
      "Episode 346: Total Reward = 64.93333333333334\n",
      "Episode 347: Total Reward = 64.33333333333334\n",
      "Episode 348: Total Reward = 214.43333333333334\n",
      "Episode 349: Total Reward = 108.36666666666686\n",
      "Episode 350: Total Reward = 297.09999999999974\n",
      "Episode 351: Total Reward = 18.499999999999996\n",
      "Episode 352: Total Reward = 133.80000000000007\n",
      "Episode 353: Total Reward = 4.76666666666666\n",
      "Episode 354: Total Reward = 451.0499999999999\n",
      "Episode 355: Total Reward = 579.4166666666665\n",
      "Episode 356: Total Reward = 478.81666666666644\n",
      "Episode 357: Total Reward = 114.63333333333328\n",
      "Episode 358: Total Reward = 468.0833333333331\n",
      "Episode 359: Total Reward = 315.8833333333331\n",
      "Episode 360: Total Reward = 529.9499999999994\n",
      "Episode 361: Total Reward = 494.78333333333285\n",
      "Episode 362: Total Reward = 310.74999999999966\n",
      "Episode 363: Total Reward = 665.5833333333333\n",
      "Episode 364: Total Reward = 236.16666666666643\n",
      "Episode 365: Total Reward = 475.0166666666663\n",
      "Episode 366: Total Reward = 446.04999999999967\n",
      "Episode 367: Total Reward = 143.9666666666667\n",
      "Episode 368: Total Reward = 78.6666666666667\n",
      "Episode 369: Total Reward = 79.13333333333333\n",
      "Episode 370: Total Reward = 58.566666666666684\n",
      "Episode 371: Total Reward = 171.3000000000001\n",
      "Episode 372: Total Reward = 60.800000000000026\n",
      "Episode 373: Total Reward = 172.2666666666666\n",
      "Episode 374: Total Reward = 238.59999999999997\n",
      "Episode 375: Total Reward = 243.8\n",
      "Episode 376: Total Reward = 209.23333333333326\n",
      "Episode 377: Total Reward = 57.03333333333333\n",
      "Episode 378: Total Reward = 57.56666666666667\n",
      "Episode 379: Total Reward = 336.8666666666664\n",
      "Episode 380: Total Reward = 41.99999999999998\n",
      "Episode 381: Total Reward = 48.00000000000001\n",
      "Episode 382: Total Reward = 39.16666666666665\n",
      "Episode 383: Total Reward = 46.366666666666674\n",
      "Episode 384: Total Reward = 39.63333333333333\n",
      "Episode 385: Total Reward = 118.23333333333338\n",
      "Episode 386: Total Reward = 186.7333333333334\n",
      "Episode 387: Total Reward = 185.53333333333333\n",
      "Episode 388: Total Reward = 303.1999999999998\n",
      "Episode 389: Total Reward = 202.6333333333332\n",
      "Episode 390: Total Reward = 40.46666666666667\n",
      "Episode 391: Total Reward = 218.53333333333336\n",
      "Episode 392: Total Reward = 40.0\n",
      "Episode 393: Total Reward = 31.26666666666667\n",
      "Episode 394: Total Reward = 220.06666666666652\n",
      "Episode 395: Total Reward = 319.9999999999999\n",
      "Episode 396: Total Reward = 144.3\n",
      "Episode 397: Total Reward = 121.26666666666672\n",
      "Episode 398: Total Reward = 273.3999999999998\n",
      "Episode 399: Total Reward = 189.1\n",
      "Episode 400: Total Reward = 523.0499999999995\n",
      "Episode 401: Total Reward = 222.56666666666678\n",
      "Episode 402: Total Reward = 239.03333333333327\n",
      "Episode 403: Total Reward = 190.3333333333333\n",
      "Episode 404: Total Reward = 230.53333333333313\n",
      "Episode 405: Total Reward = 690.5166666666668\n",
      "Episode 406: Total Reward = 617.7499999999997\n",
      "Episode 407: Total Reward = 63.76666666666668\n",
      "Episode 408: Total Reward = 61.933333333333344\n",
      "Episode 409: Total Reward = 343.7000000000001\n",
      "Episode 410: Total Reward = 400.1499999999999\n",
      "Episode 411: Total Reward = 66.2\n",
      "Episode 412: Total Reward = 145.4333333333334\n",
      "Episode 413: Total Reward = 8.066666666666652\n",
      "Episode 414: Total Reward = 156.83333333333331\n",
      "Episode 415: Total Reward = 102.83333333333337\n",
      "Episode 416: Total Reward = 78.96666666666671\n",
      "Episode 417: Total Reward = 179.1\n",
      "Episode 418: Total Reward = 31.06666666666667\n",
      "Episode 419: Total Reward = 86.03333333333336\n",
      "Episode 420: Total Reward = 8.599999999999998\n",
      "Episode 421: Total Reward = 22.200000000000003\n",
      "Episode 422: Total Reward = 140.29999999999998\n",
      "Episode 423: Total Reward = 222.2666666666667\n",
      "Episode 424: Total Reward = 238.79999999999984\n",
      "Episode 425: Total Reward = 168.93333333333325\n",
      "Episode 426: Total Reward = -49.699999999999996\n",
      "Episode 427: Total Reward = 155.46666666666658\n",
      "Episode 428: Total Reward = 203.20000000000002\n",
      "Episode 429: Total Reward = -19.83333333333333\n",
      "Episode 430: Total Reward = 154.50000000000003\n",
      "Episode 431: Total Reward = -51.03333333333332\n",
      "Episode 432: Total Reward = -51.366666666666674\n",
      "Episode 433: Total Reward = 415.88333333333316\n",
      "Episode 434: Total Reward = 47.399999999999984\n",
      "Episode 435: Total Reward = 153.79999999999995\n",
      "Episode 436: Total Reward = 235.59999999999974\n",
      "Episode 437: Total Reward = 69.53333333333332\n",
      "Episode 438: Total Reward = 65.70000000000005\n",
      "Episode 439: Total Reward = 369.6999999999997\n",
      "Episode 440: Total Reward = 49.300000000000004\n",
      "Episode 441: Total Reward = 150.16666666666646\n",
      "Episode 442: Total Reward = 195.69999999999993\n",
      "Episode 443: Total Reward = 91.53333333333332\n",
      "Episode 444: Total Reward = 62.033333333333324\n",
      "Episode 445: Total Reward = 108.20000000000003\n",
      "Episode 446: Total Reward = 104.96666666666692\n",
      "Episode 447: Total Reward = 368.2499999999997\n",
      "Episode 448: Total Reward = 57.53333333333333\n",
      "Episode 449: Total Reward = 253.9666666666667\n",
      "Episode 450: Total Reward = 170.9333333333334\n",
      "Episode 451: Total Reward = 46.06666666666666\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain_dqn_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m, in \u001b[0;36mtrain_dqn_agent\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# environment responds to the action (state is changed)\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# gained experience added to replay buffer and agent is updated\u001b[39;00m\n\u001b[0;32m     24\u001b[0m agent\u001b[38;5;241m.\u001b[39madd_experience(state, action, reward, next_state, done)\n",
      "Cell \u001b[1;32mIn[2], line 74\u001b[0m, in \u001b[0;36mPongEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_paddle\u001b[38;5;241m.\u001b[39msety(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_paddle\u001b[38;5;241m.\u001b[39mycor() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Move the ball\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mball\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mball\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxcor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mball\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mball\u001b[38;5;241m.\u001b[39msety(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mball\u001b[38;5;241m.\u001b[39mycor() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mball\u001b[38;5;241m.\u001b[39mdy)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Ball collision with top and bottom walls\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda32\\envs\\It_Works\\lib\\turtle.py:1808\u001b[0m, in \u001b[0;36mTNavigator.setx\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetx\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set the turtle's first coordinate to x\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m \n\u001b[0;32m   1795\u001b[0m \u001b[38;5;124;03m    Argument:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;124;03m    (10.00, 240.00)\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1808\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_goto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVec2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_position\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda32\\envs\\It_Works\\lib\\turtle.py:3159\u001b[0m, in \u001b[0;36mRawTurtle._goto\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m   3151\u001b[0m go_modes \u001b[38;5;241m=\u001b[39m ( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drawing,\n\u001b[0;32m   3152\u001b[0m              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pencolor,\n\u001b[0;32m   3153\u001b[0m              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pensize,\n\u001b[0;32m   3154\u001b[0m              \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fillpath, \u001b[38;5;28mlist\u001b[39m))\n\u001b[0;32m   3155\u001b[0m screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\n\u001b[0;32m   3156\u001b[0m undo_entry \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_position, end, go_modes,\n\u001b[0;32m   3157\u001b[0m               (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentLineItem,\n\u001b[0;32m   3158\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentLine[:],\n\u001b[1;32m-> 3159\u001b[0m               \u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pointlist\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrentLineItem\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3160\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[:])\n\u001b[0;32m   3161\u001b[0m               )\n\u001b[0;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mundobuffer:\n\u001b[0;32m   3163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mundobuffer\u001b[38;5;241m.\u001b[39mpush(undo_entry)\n",
      "File \u001b[1;32md:\\anaconda32\\envs\\It_Works\\lib\\turtle.py:753\u001b[0m, in \u001b[0;36mTurtleScreenBase._pointlist\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pointlist\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m    746\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"returns list of coordinate-pairs of points of item\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m    Example (for insiders):\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    >>> from turtle import *\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;124;03m    (9.9999999999999982, 0.0)]\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m    >>> \"\"\"\u001b[39;00m\n\u001b[1;32m--> 753\u001b[0m     cl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m     pl \u001b[38;5;241m=\u001b[39m [(cl[i], \u001b[38;5;241m-\u001b[39mcl[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(cl), \u001b[38;5;241m2\u001b[39m)]\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  pl\n",
      "File \u001b[1;32m<string>:1\u001b[0m, in \u001b[0;36mcoords\u001b[1;34m(self, *args, **kw)\u001b[0m\n",
      "File \u001b[1;32md:\\anaconda32\\envs\\It_Works\\lib\\tkinter\\__init__.py:2795\u001b[0m, in \u001b[0;36mCanvas.coords\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2791\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of coordinates for the item given in ARGS.\"\"\"\u001b[39;00m\n\u001b[0;32m   2792\u001b[0m \u001b[38;5;66;03m# XXX Should use _flatten on args\u001b[39;00m\n\u001b[0;32m   2793\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk\u001b[38;5;241m.\u001b[39mgetdouble(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m   2794\u001b[0m                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk\u001b[38;5;241m.\u001b[39msplitlist(\n\u001b[1;32m-> 2795\u001b[0m            \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)]\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_dqn_agent()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "It_Works",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
